{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b0bef8-efa9-4cf8-a094-1998281d4e9d",
   "metadata": {},
   "source": [
    "1)Scrape all product names and prices from the first two pages of \"Books to Scrape\" (http://books.toscrape.com/). Handle simple pagination and structure the output as a list of dictionaries with 'title' and 'price'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27887eba-3d1d-4ac3-b7eb-7bcf294e872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total books scraped: 40\n",
      "[{'title': 'A Light in the Attic', 'price': '£51.77'}, {'title': 'Tipping the Velvet', 'price': '£53.74'}, {'title': 'Soumission', 'price': '£50.10'}, {'title': 'Sharp Objects', 'price': '£47.82'}, {'title': 'Sapiens: A Brief History of Humankind', 'price': '£54.23'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
    "books_list = []\n",
    "\n",
    "for page_num in range(1, 3):\n",
    "    url = base_url.format(page_num)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    response.encoding = 'utf-8'\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Could not retrieve page {page_num}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    products = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "    for product in products:\n",
    "        title = product.h3.a['title']\n",
    "        price = product.find('p', class_='price_color').text\n",
    "        books_list.append({'title': title, 'price': price})\n",
    "\n",
    "print(f\"Total books scraped: {len(books_list)}\")\n",
    "print(books_list[:5])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75353968-ca66-49e9-ac5c-61a4b88b9894",
   "metadata": {},
   "source": [
    "\n",
    "2)Extract the current weather descriptions (like ‘clear’, ‘cloudy’) and temperatures for at least five cities from a public weather site (such as https://www.weather.com or https://wttr.in). Present your data in a tabular format (city, description, temperature).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85285000-ecf4-4679-9c43-39f9aa33b7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       City               Description Temperature (°C)\n",
      "0    London                     Sunny               23\n",
      "1  New York                     Clear               26\n",
      "2     Tokyo             Partly cloudy               28\n",
      "3    Sydney  Thunderstorm in vicinity               12\n",
      "4     Paris                     Clear               18\n",
      "Weather data saved to 'weather_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "cities = [\"London\", \"New York\", \"Tokyo\", \"Sydney\", \"Paris\"]\n",
    "weather_data = []\n",
    "\n",
    "for city in cities:\n",
    "    url = f\"https://wttr.in/{city}?format=j1\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Could not get weather for {city}\")\n",
    "        continue\n",
    "    \n",
    "    data = response.json()\n",
    "    current_condition = data['current_condition'][0]\n",
    "    description = current_condition['weatherDesc'][0]['value']\n",
    "    temperature_c = current_condition['temp_C']\n",
    "    \n",
    "    weather_data.append({\n",
    "        'City': city,\n",
    "        'Description': description,\n",
    "        'Temperature (°C)': temperature_c\n",
    "    })\n",
    "\n",
    "df_weather = pd.DataFrame(weather_data)\n",
    "print(df_weather)\n",
    "\n",
    "df_weather.to_csv('weather_data.csv', index=False)\n",
    "print(\"Weather data saved to 'weather_data.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff36f580-1f86-4245-9b84-edb1dca36dd2",
   "metadata": {},
   "source": [
    "3)From the “Real Python Fake Jobs” board (https://realpython.github.io/fake-jobs/), gather all job titles, companies, and locations listed on the first three pages. Save the results as a CSV file. Be sure to loop through the pagination and properly parse the HTML for structured data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4026e6-6967-43b1-b222-dc91b77a2ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: https://realpython.github.io/fake-jobs/page/1/\n",
      "Couldn't access page 1\n",
      "Scraping page: https://realpython.github.io/fake-jobs/page/2/\n",
      "Couldn't access page 2\n",
      "Scraping page: https://realpython.github.io/fake-jobs/page/3/\n",
      "Couldn't access page 3\n",
      "Saved 0 job listings to 'fake_jobs.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://realpython.github.io/fake-jobs/page/{}/\"\n",
    "\n",
    "all_jobs = []\n",
    "\n",
    "\n",
    "for page in range(1, 4):\n",
    "    url = base_url.format(page)\n",
    "    print(f\"Scraping page: {url}\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Couldn't access page {page}\")\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    job_cards = soup.find_all(\"div\", class_=\"card-content\")\n",
    "    \n",
    "    for card in job_cards:\n",
    "        title = card.find(\"h2\", class_=\"title\").get_text(strip=True)\n",
    "        company = card.find(\"h3\", class_=\"company\").get_text(strip=True)\n",
    "        location = card.find(\"p\", class_=\"location\").get_text(strip=True)\n",
    "        \n",
    "        all_jobs.append({\n",
    "            \"Title\": title,\n",
    "            \"Company\": company,\n",
    "            \"Location\": location\n",
    "        })\n",
    "\n",
    "df_jobs = pd.DataFrame(all_jobs)\n",
    "\n",
    "df_jobs.to_csv(\"fake_jobs.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(df_jobs)} job listings to 'fake_jobs.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b4d07-cda4-4a7e-a626-bbbb884c5cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
